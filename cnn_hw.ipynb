{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ef5f34-c1a5-4659-a901-ce1dedcc653e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "070c18b3-2607-46c5-872a-0a70c213eea4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e93dd61-252c-48b9-91bd-3633195894cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9158354e-f426-43d5-84dc-0532a69a5b50",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "353d67cc-76e9-4abe-b4f2-8c0bc4dea347",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, criterion, optimizer, n_epoch):\n",
    "    for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "    \n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5872bcae-eb94-48f1-890e-be0b02c91f6f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(model, criterion, optimizer, batch_size, n_epoch=2):\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "    train_model(model, trainloader, criterion, optimizer, n_epoch)\n",
    "    test_model(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def test_model(model, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "cbfc8f08-e578-4092-8399-1c2496d8975b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "в качестве функции потерь рассмотрим кросс-энтропию, в качестве метода оптимизации весов - стохастический градиентный спуск.\n",
    "Будем менять гипер-параметры оптимизатора, размер батча, количество эпох."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed8b7e81-1134-468a-9de2-37d443a48334",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.191\n",
      "[1,  4000] loss: 1.859\n",
      "[1,  6000] loss: 1.718\n",
      "[1,  8000] loss: 1.647\n",
      "[1, 10000] loss: 1.532\n",
      "[1, 12000] loss: 1.471\n",
      "[2,  2000] loss: 1.416\n",
      "[2,  4000] loss: 1.358\n",
      "[2,  6000] loss: 1.346\n",
      "[2,  8000] loss: 1.308\n",
      "[2, 10000] loss: 1.292\n",
      "[2, 12000] loss: 1.264\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 53 %\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "run_model(net, criterion, optimizer, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68092e22-3428-48eb-b002-1091d0816443",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.220\n",
      "[1,  4000] loss: 1.788\n",
      "[1,  6000] loss: 1.576\n",
      "[2,  2000] loss: 1.456\n",
      "[2,  4000] loss: 1.393\n",
      "[2,  6000] loss: 1.346\n",
      "[3,  2000] loss: 1.256\n",
      "[3,  4000] loss: 1.257\n",
      "[3,  6000] loss: 1.212\n",
      "[4,  2000] loss: 1.160\n",
      "[4,  4000] loss: 1.137\n",
      "[4,  6000] loss: 1.134\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 60 %\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "run_model(net, criterion, optimizer, batch_size, n_epoch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.776\n",
      "[1,  4000] loss: 1.499\n",
      "[1,  6000] loss: 1.418\n",
      "[2,  2000] loss: 1.318\n",
      "[2,  4000] loss: 1.294\n",
      "[2,  6000] loss: 1.258\n",
      "[3,  2000] loss: 1.189\n",
      "[3,  4000] loss: 1.173\n",
      "[3,  6000] loss: 1.172\n",
      "[4,  2000] loss: 1.078\n",
      "[4,  4000] loss: 1.124\n",
      "[4,  6000] loss: 1.116\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 59 %\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "run_model(net, criterion, optimizer, batch_size, n_epoch=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.839\n",
      "[1,  4000] loss: 1.563\n",
      "[1,  6000] loss: 1.525\n",
      "[1,  8000] loss: 1.438\n",
      "[1, 10000] loss: 1.423\n",
      "[1, 12000] loss: 1.400\n",
      "[2,  2000] loss: 1.339\n",
      "[2,  4000] loss: 1.312\n",
      "[2,  6000] loss: 1.307\n",
      "[2,  8000] loss: 1.303\n",
      "[2, 10000] loss: 1.296\n",
      "[2, 12000] loss: 1.279\n",
      "[3,  2000] loss: 1.192\n",
      "[3,  4000] loss: 1.209\n",
      "[3,  6000] loss: 1.217\n",
      "[3,  8000] loss: 1.216\n",
      "[3, 10000] loss: 1.208\n",
      "[3, 12000] loss: 1.212\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 55 %\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "run_model(net, criterion, optimizer, batch_size, n_epoch=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Качество модели среднее, для улучшения модели можно применить более сложные свертки и пулинги или использовать предобученные модели. В рамках данной модели изменение количества эпох (увеличение количества), оптимизатора (подобрать более гибкий) в динамике показывают лучшие показатели обучения."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}